下面用严谨的、可直接用于写代码/复现的结构化语言描述 Mamba-FSCIL。我严格基于论文原文，不改变原意。每个关键点后我都给出论文文件引用，方便你对照原文。

Li 等 - 2025 - Mamba-FSCIL Dynam…

组件（模块化）

Backbone f

功能：把输入图像 x 映射为中间特征 F。

可训练性：仅在 base session（t=0）训练并优化，增量(session>0)时冻结。用途：提供稳定的通用特征。

Dual Selective SSM Projector（投影头，总体输出 µ）

由三条分支组成（最终表示按元素相加）：

piden — Identity branch，简单池化/残差路径，输出 µ_iden（固定或随 base 训练）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

g_base — Base selective SSM branch：在 base session 训练，随后冻结，输出 µ_base（确保老类稳定）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

g_inc — Incremental selective SSM branch：从第一个增量会话开始引入，仅在增量会话训练，输出 µ_inc（学习新类的偏移/适配）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

最终表示： µ = µ_iden + µ_base + µ_inc（随后做 L2 归一化用于分类）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

Selective SSM Branch 内部（g_base / g_inc 共享结构）

两条内部路径：

Scan Path：基于 Mamba (Selective SSM) 的 SSM 扫描过程。关键点：输入依赖参数 Bτ = fB(x̂), Cτ = fC(x̂), ∆τ = f∆(x̂)，这些由小型网络（例如线性层 / MLP）从投影后的补丁/局部特征生成，然后进行 SSM (Mamba/SS2D) scan 运算得到分支输出。

Gate Path：从相同或并行的特征生成门控向量 Z（经 SiLU），用于按位缩放/加权 Scan Path 输出（控制分支对最终表示的贡献）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

Classifier：Fixed ETF + Dot Regression (DR) Loss

分类器 ŴETF 是固定的等角框架（ETF）原型矩阵（不训练/不更新）。分类损失采用 dot-regression (DR) loss（论文 Eq.(27)）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

Memory M(t)

存储每个已见类的类均值特征（embedding center，L2 归一化后的 µ̂），用于在增量会话中作为重放/锚点（每类 1 个均值）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

损失函数（精确公式 & 计算流程）

我把论文中最重要的公式以实现友好的方式列出（变量名与论文保持一致）。

Dot-Regression (DR) 分类损失（用于所有训练阶段，Eq.(27)）：
对样本 i 的最终 L2 归一化表示 µ̂_i 和其 ETF 原型 ŵ_yi：

L_DR(µ̂_i, ŴETF) = 0.5 * ( ŵ_yi^T µ̂_i - 1 )^2


在 batch 上取平均作为 L_cls 的一部分。

Li 等 - 2025 - Mamba-FSCIL Dynam…

Suppression Loss (L_supp)（类敏感，用于 g_inc 引导，目标：抑制 g_inc 对 base 输入的贡献、放大对 novel 输入的贡献）——论文在 Fig.2(c) 与 Sec.4.3 描述：

对于 g_inc 的 gate 输出 Zinc，分别对 base 类样本（目标范数较小）和 novel 类样本（目标范数较大）施加 L2-norm 约束。

形式（伪数学）：

L_supp_base(t)  = 1/|B_base| * Σ || Zinc(x) ||_2 - τ_base ||^2    for base inputs in batch
L_supp_novel(t) = 1/|B_novel| * Σ || Zinc(x) ||_2 - τ_novel ||^2  for novel inputs in batch


其中 τ_base < τ_novel（论文通过超参/目标值控制）。论文给出 λ1, λ2 控制比重。

Separation Loss (L_sep)（促使 base 与 novel 的输入依赖参数（B,C,∆）在参数空间区分）——论文 Eq.(25–26) 的意图：

先分别对 batch 中 base / novel 输入计算动态参数的平均向量（例如 \bar{B}_b, \bar{C}_b, \bar{Δ}_b 与 \bar{B}_n, ...）。

计算它们之间的角距/余弦相似度并最大化角度（等价最小化绝对余弦相似度）。

伪公式（实现时按通道 flatten 后求平均再算 cosine）：

sim_B = cosine( flatten( mean_B_base ), flatten( mean_B_novel ) )
sim_C = cosine( flatten( mean_C_base ), flatten( mean_C_novel ) )
sim_Δ = cosine( flatten( mean_Δ_base ), flatten( mean_Δ_novel ) )
L_sep = |sim_B| + |sim_C| + |sim_Δ|


用 λ3 乘以该项加入总损失。

增量会话的总目标（仅优化 g_inc）（论文 Eq.(30)）：

min_{g_inc}   L_cls(t) + λ1 * L_supp_base(t) + λ2 * L_supp_novel(t) + λ3 * L_sep(t)


其中 L_cls(t) 包含当前会话样本与 memory 的 DR 损失（论文 Eq.(29)）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

训练步骤（伪代码风格，便于直接实现）
# 假设：D(0) = base dataset, D(t) incremental datasets
# Modules: backbone f, piden, g_base, g_inc (new instance), classifier ETF
# Memory M = {} initially empty

# --- Base Session (t = 0) ---
# initialize g_base, piden, and backbone f
for epoch in range(base_epochs):
    for batch in dataloader(D0, batch_size=base_batch):
        F = f(batch.x)                 # backbone (trainable here)
        mu_iden = piden(F)
        mu_base = g_base(F)           # SSM branch
        mu = mu_iden + mu_base + zero_like(mu_iden)  # g_inc zero at base
        mu_hat = l2_normalize(mu)
        loss = mean( L_DR(mu_hat, ETF) )
        backprop_and_step(loss, params=[f, piden, g_base, ETF?])  # ETF fixed if using fixed ETF

# compute and store class centers into memory M(0)
for each class c in C(0):
    M[c] = mean_l2_normalized_representation_of_class_c

# freeze backbone, piden, g_base
freeze_parameters([f, piden, g_base])

# --- Incremental Sessions (t = 1..T) ---
for t in range(1, T+1):
    # create new incremental branch g_inc_t (or reuse g_inc but zero-init gating)
    g_inc = initialize_g_inc_zero_gate()  # ensure its initial µ_inc = 0 (paper: zero init)
    for iter in range(incremental_iters):
        # sample minibatch from D(t) U M(t) (memory provides class centers)
        batch = sample_mini_batch(D[t], M, batch_size=inc_batch)
        F = f(batch.x)  # backbone frozen -> no grads
        mu_iden = piden(F)        # frozen
        mu_base = g_base(F)       # frozen
        mu_inc = g_inc(F)         # learnable
        mu = mu_iden + mu_base + mu_inc
        mu_hat = l2_normalize(mu)

        # classification loss on current data and memory (DR)
        L_cls = mean_DR_on_current_and_memory(mu_hat, labels, ETF)

        # compute L_supp: for base vs novel samples in this batch, operate on Zinc norms
        L_supp_base  = mean( (norm(Zinc_on_base) - tau_base)^2 )
        L_supp_novel = mean( (norm(Zinc_on_novel) - tau_novel)^2 )

        # compute L_sep: average generated B,C,Δ for base vs novel in batch -> cosine sims
        L_sep = |cosine(mean_B_base, mean_B_novel)| + ... (for C and Δ)

        loss = L_cls + lambda1*L_supp_base + lambda2*L_supp_novel + lambda3*L_sep
        backprop_and_step(loss, params=[g_inc_only])

    # update memory M with new class centers (add centers for new classes)
    for each new class c in C(t):
        M[c] = compute_class_center_using_current_model(c)


关键实现细节（来自论文）：

Zero initialization of g_inc 的 gate projection f_z，确保 µ_inc 初始为 0，不影响已学表示。

Li 等 - 2025 - Mamba-FSCIL Dynam…

只优化 g_inc（增量阶段），backbone, piden, g_base 都被冻结（论文在多个节强调）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

超参数 / 维度 /训练设置（论文中给出的可直接复现值）

SSM 参数维度：D_B = D_C = D_Δ = 256（论文 Eq.(13) 的默认），SSM 输出维 D'：miniImageNet 使用 1024，CIFAR-100/CUB-200 使用 512（见实验节）。

训练批次 / 学习率 /迭代数（参考实现）：

Base session：batch=512（论文默认），ResNet-12 在 miniImageNet 上训练 500 epochs；learning rate base 0.25（示例）。

Incremental session：batch=64，100–170 iterations（miniImageNet 示例），初始 lr 0.01（或论文表中的具体设置），使用 SGD + momentum 0.9（论文记载为 0.009? — 原文写 0.009，通常可能是 0.9，请核对原段落）。学习率采用 cosine annealing。

损失系数范围（论文给的搜索范围）：

λ1, λ2（suppression）与 λ3（separation）分别在经验范围：λ1/λ2 ≈ 50–200 或 0.001–1（论文列出不同项的范围），λ3 ≈ 0.05–0.5。你在复现时需要在这些范围内网格/随机搜索以找到最稳的组合（论文做了敏感性分析）。

注：论文中有若干具体 lr / epoch / iteration 在不同 backbone / 数据集间的微调值，复现实验时按对应表设置（见 Sec.5.1）。

实验设计（数据集、会话设置、评估指标、消融）

数据集：miniImageNet（100 类，base=60，8 sessions of 5-way 5-shot），CIFAR-100（同样设置），CUB-200（base=100，10 sessions of 10-way 5-shot）。

Li 等 - 2025 - Mamba-FSCIL Dynam…

会话流程：

Session 0：训练 base 类（大样本）并建立 ETF / g_base / backbone。

Session t (1..T)：每轮引入 p-way q-shot novel 类样本，只训练 g_inc（且从零门控初始），每轮后把新类中心加入 memory。

Baseline / 比较方法与 Ablation：论文比较了静态适配方法（如 MLP-neck）、单 Selective SSM、双分支 Selective SSM、以及加入 class-sensitive losses 的完全模型，并在同等参数预算下给出表格（Table 5）。这有助于判断每一项设计的贡献。

评估指标：

每个 session 的分类精度（Session-wise accuracy）

AVG（所有 session 的平均精度）

PD（Performance Drop / accuracy drop 从第 0 会话到最后会话）

最后一会话的精度（Final）等。论文报告 AVG、FINAL、∆AVG、∆FINAL 等对比数值。

实现注意事项 / 效率与工程建议

并行化 Mamba Scan（SS2D）：Mamba 的 scan 可以并行化并实现为卷积核 K = (C B, C A B, ..., C A^{L-1} B) 与输入做卷积，保证线性时间复杂度（论文 Sec.3.3）。实现时注意缓存 / vectorize 以便在 GPU 上高效运行。

Li 等 - 2025 - Mamba-FSCIL Dynam…

内存模块（class centers）：存储的是 feature center（L2 归一化），不是原始样本，内存小且高效。更新策略：每次增量会话完成后计算新类中心并加入 M。

Li 等 - 2025 - Mamba-FSCIL Dynam…

g_inc 的零初始化：务必将 gate 投影 f_z 的参数初始化为 0（或使 µ_inc 初始为 0），以免刚开始训练时破坏 base 表示。论文特别指出此点。

Li 等 - 2025 - Mamba-FSCIL Dynam…

冻结策略：严格冻结 f, piden, g_base；仅 g_inc 可训练（避免无意中微调 backbone 导致灾难性遗忘）。论文的 ablation 也考察了其他冻结策略。